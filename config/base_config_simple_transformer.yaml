# Run info
run:
  run_name: clip_prefix_simple_transformer_crossattn
  seed: 42
  output_dir: ./runs/clip_prefix_simple_transformer_crossattn
  log_every_n_steps: 10

# Model (ClipLM + Cross Attn)
model_args:
  vocab_size: 8195
  block_size: 1024
  n_layer: 6
  n_head: 8
  n_embd: 512
  dropout: 0.1
  clip_dim: 768
  prefix_len: 8
  pad_token: 8192
  bias: false

# Dataset / DataModule
data_args:
  embeddings_root: /Users/swornimchhetri/Desktop/all_codes/github_stuff/Image-Captioning/embeddings/token_embedding
  train_csv: /Users/swornimchhetri/Desktop/all_codes/github_stuff/Image-Captioning/csvs/coco_train_tok.csv
  val_csv: /Users/swornimchhetri/Desktop/all_codes/github_stuff/Image-Captioning/csvs/coco_val_tok.csv
  max_len: 69 # This is the max text tokens right now
  pad_id: 8192
  batch_size: 256
  num_workers: 0

# Optimization
optim_args:
  lr: 3e-4
  min_lr: 1e-6         # floor for cosine decay
  weight_decay: 0.1
  betas: [0.9, 0.95]
  warmup_steps: 1_000

# Training args
train_args:
  accelerator: mps 
  strategy: auto          
  max_steps: 20_000
  log_every_n_steps: 10
  val_check_interval: 2_000 
  gradient_clip_val: 1.0
  deterministic: true